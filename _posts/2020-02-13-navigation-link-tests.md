---
title: Checking Navigation Link Uptime
subtitle: using JavaScript 
image: /img/james-lee-c0HH4m75jYA-unsplash.jpg
tags: [javascript]
---

Today, I received a request to dynamically check the HTTP response code of every sub-category link on the site's navigation menu using synthetic test agents. Categories included Shoes, Home, Beauty, Kids, Women, Men, etc. and sub-categories included more specific links such as Boots, Sandals, Slippers, Trainers, etc. This client wanted to set these tests up under a third-party service, so the options were limited to either (1) creating a single page measurement for each sub-category link, or (2) creating a multi-page script to test all of the sub-category links. So, any sort of web crawling or web scraping service was not an option. Setting up single page measurements would not work since this client had a dynamic sub-category list that would change over time. So, that left me with the multi-page script option. The third-party does support some Selenium commands in their multi-step synthetic scripts, which is quite fortunate. 

However, the provider's multi-step synthetic scripts stop the test completely when there is one failure. So if the second sub-category link returns a 404 code, then the entire script will stop and not test the remaining sub-categories. Crawling through each sub-category link was not an option with this configuration. So, I had to think outside of the box to fulfill their requirements. 

Since the customer is only interested in the HTTP response code, the request was substantially simpler than I originally thought. For a concept test, I started with testing all of the sub-category links for only one category page. The customer wanted a high-level pass/fail response for each category, and then detailed failure messages for each sub-category link in the console log. So, I wrote a small snippet of JavaScript to pull all of the sub-category links into a list, and then asynchronously request each URL through an XHR request for the response code. This snippet also included a separate section to print a custom message to the page that had the number of failures and a particular CSS class if there were any failures. The synthetic script opened the category URL, ran the JavaScript snippet, and then checked for that particular CSS class to pass/fail the script as a whole. 
```
// Pull list of sub-category links 

// Get the HTTP code for each link 

// Print a custom message to the page 
```

Once that concept test was up and running successfully in their third-party software, I was able to wrap up the project by expanding to other category pages. Essentially, this client occassionally changed their category URLs or added a new category (such as a holiday or Mother's Day category) so those needed to be tested dynamically as well. I had already built a piece to test all of the sub-category links, but as mentioned before the test would stop completely if a link on one of those category pages failed. So instead I added a separate JavaScript snippet to map each category link to a minute of the hour, and then the test would run once per minute constantly. There would be slightly fewer tests per hour run on a couple of the category links, but the client's requirements included testing each category page every 10 minutes, so this was sufficient. 

<!--
Sometimes we get requests from clients with a lot of constraints, and we have to do the best with the constraints given. This was one such project. Essentially, the customer wanted to crawl through all of their category and subcategory links to check the response code of each one. Typically customers would create one base page measurement for each link, but they needed the list of subcategories to be generated dynamically. They could also use a spider or crawling software, but they wanted this to be run by their third party synthetic software, which limited our options substantially. 
The solution ended up involving mapping each category page to each minute of the hour and then running a test every minute to open that category page and check all the subcategory links on it. This custom software only supported JavaScript, so we could not click on each subcategory link (when you click on a link in JavaScript the entire page reloads and thus ceases any JavaScript execution). Since the client was only interested in the HTTP response code of each link, we were able to instead make an XHR request for each subcategory link. The client's requirements included (1) failing the synthetic measurement when any links responded with a HTTP code greater than 400, (2) printing the number of failures and the URL of each failed link to the page for screenshot, and (3) generating a console log output with the response code and URL of each subcategory link. 
I fully recognize that this is not an ideal solution or even a great solution, but it met the clients requirements and was up and running within an hour. Essentially someone on the client's business team needed to be verifying all of these links every 10 minutes and generating an alert whenever it failed, so we needed a fast and functional solution. This was an acceptable work-around that integrated with their third-party solution for the next few months until that third party provider can add a crawling and scraping feature. So this project was much shorter and quicker than usual, but it helped the client substantially and fit within their restrictions. 
-->
